{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "%pip install pandas numpy torch opencv-python torchvision tqdm scikit_learn\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "import cv2\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>flow_rate</th>\n",
       "      <th>feed_rate</th>\n",
       "      <th>z_offset</th>\n",
       "      <th>target_hotend</th>\n",
       "      <th>hotend</th>\n",
       "      <th>bed</th>\n",
       "      <th>nozzle_tip_x</th>\n",
       "      <th>nozzle_tip_y</th>\n",
       "      <th>print_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caxton_dataset/print0/image-1.jpg</td>\n",
       "      <td>2020-10-08T13:12:48-02</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>204.34</td>\n",
       "      <td>65.66</td>\n",
       "      <td>531</td>\n",
       "      <td>554</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>caxton_dataset/print0/image-2.jpg</td>\n",
       "      <td>2020-10-08T13:12:48-48</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>204.34</td>\n",
       "      <td>65.66</td>\n",
       "      <td>531</td>\n",
       "      <td>554</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>caxton_dataset/print0/image-3.jpg</td>\n",
       "      <td>2020-10-08T13:12:48-94</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>204.13</td>\n",
       "      <td>65.74</td>\n",
       "      <td>531</td>\n",
       "      <td>554</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            img_path               timestamp  flow_rate  \\\n",
       "0  caxton_dataset/print0/image-1.jpg  2020-10-08T13:12:48-02        100   \n",
       "1  caxton_dataset/print0/image-2.jpg  2020-10-08T13:12:48-48        100   \n",
       "2  caxton_dataset/print0/image-3.jpg  2020-10-08T13:12:48-94        100   \n",
       "\n",
       "   feed_rate  z_offset  target_hotend  hotend    bed  nozzle_tip_x  \\\n",
       "0        100       0.0          205.0  204.34  65.66           531   \n",
       "1        100       0.0          205.0  204.34  65.66           531   \n",
       "2        100       0.0          205.0  204.13  65.74           531   \n",
       "\n",
       "   nozzle_tip_y  print_id  \n",
       "0           554         0  \n",
       "1           554         0  \n",
       "2           554         0  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = 'data/caxton_dataset'\n",
    "\n",
    "caxton = pd.read_csv(os.path.join(dataset_path,'caxton_dataset_full.csv'))\n",
    "# caxton = caxton[(caxton['print_id'] < 1) | (caxton['print_id'] > 190)]\n",
    "caxton.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 30 20:33:29 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1080 Ti     On  |   00000000:88:00.0 Off |                  N/A |\n",
      "| 27%   47C    P2             71W /  250W |    7914MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    579174      C   ...haiza/miniconda3/envs/ka/bin/python       7910MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 584.12 MB\n",
      "GPU memory cached: 977.27 MB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "clear_gpu_memory()\n",
    "\n",
    "print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1e6:.2f} MB\")\n",
    "print(f\"GPU memory cached: {torch.cuda.memory_reserved()/1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_label(row):\n",
    "    print_id = row['print_id']\n",
    "    return 1 if print_id >= 183 else 0  # 1 for failure, 0 for non-failure\n",
    "\n",
    "caxton['failure'] = caxton.apply(create_binary_label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='failure'>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAG6CAYAAADaq0anAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAf3UlEQVR4nO3de1DVdf7H8dcB8mDqOS6gR6ijYjs2zGKmx7ZAmc02KXKd3LWVHXcjTWdjcjMlm5Hc6eI0Uf3UqM1bq+Q4Y8a43YvMMzWGl3ZWCfYmu+Wtgwqx2ARKLSic3x+O7JwA5SDy7uDzMXP+OF8+33PeZybi6ff7Pec4gsFgUAAAAEairAcAAACXN2IEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJiKqBgpLS3VtGnTlJSUJIfDoTfffDPsxwgGg1q+fLlGjx4tp9Mpr9erp556queHBQAAXRJjPUA4GhsbNXbsWM2ZM0czZszo1mM8+OCD2r59u5YvX64xY8aovr5edXV1PTwpAADoKkekflGew+HQG2+8oenTp7dta25u1u9//3tt3rxZX3/9tVJTU/XMM8/o5ptvliRVVlbquuuu0z/+8Q9de+21NoMDAIAQEXWa5kLmzJmj3bt369VXX9Xf/vY3/fKXv9Ttt9+uzz//XJL0zjvvaNSoUXr33XeVnJyskSNHat68efrqq6+MJwcA4PLVZ2Lk4MGD2rJli7Zu3aqMjAxdc801Wrx4sSZNmqSXX35ZknTo0CF98cUX2rp1qzZt2qSNGzeqrKxMd911l/H0AABcviLqmpHz+fTTTxUMBjV69OiQ7U1NTYqPj5cktba2qqmpSZs2bWpbt2HDBvl8Pv373//m1A0AAAb6TIy0trYqOjpaZWVlio6ODvnZwIEDJUmJiYmKiYkJCZaUlBRJUiAQIEYAADDQZ2Jk3LhxamlpUW1trTIyMjpcM3HiRJ05c0YHDx7UNddcI0n67LPPJEkjRozotVkBAMD/RNS7aU6dOqUDBw5IOhsfK1eu1OTJkxUXF6fhw4frN7/5jXbv3q0VK1Zo3Lhxqqur00cffaQxY8bojjvuUGtrq2644QYNHDhQhYWFam1t1fz58+VyubR9+3bjVwcAwOUpomJkx44dmjx5crvt99xzjzZu3KjTp0/rySef1KZNm3Ts2DHFx8crLS1NTzzxhMaMGSNJOn78uB544AFt375dAwYMUFZWllasWKG4uLjefjkAAEARFiMAAKDv6TNv7QUAAJEpIi5gbW1t1fHjxzVo0CA5HA7rcQAAQBcEg0GdPHlSSUlJiorq/PhHRMTI8ePH5fV6rccAAADdUFVVpauvvrrTn0dEjAwaNEjS2RfjcrmMpwEAAF3R0NAgr9fb9ne8MxERI+dOzbhcLmIEAIAIc6FLLLiAFQAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYirEeAOc3csl71iOgFx15eqr1CADQ6zgyAgAATBEjAADAFDECAABMESMAAMBU2DFSWlqqadOmKSkpSQ6HQ2+++eZ517/++uuaMmWKhgwZIpfLpbS0NH3wwQfdnRcAAPQxYcdIY2Ojxo4dqxdffLFL60tLSzVlyhSVlJSorKxMkydP1rRp01ReXh72sAAAoO8J+629WVlZysrK6vL6wsLCkPtPPfWU3nrrLb3zzjsaN25ch/s0NTWpqamp7X5DQ0O4YwIAgAjR69eMtLa26uTJk4qLi+t0TUFBgdxud9vN6/X24oQAAKA39XqMrFixQo2NjZo5c2ana/Lz81VfX992q6qq6sUJAQBAb+rVT2DdsmWLHn/8cb311lsaOnRop+ucTqecTmcvTgYAAKz0WowUFxdr7ty52rp1q2699dbeeloAAPA91yunabZs2aLZs2frlVde0dSpfPcGAAD4n7CPjJw6dUoHDhxou3/48GFVVFQoLi5Ow4cPV35+vo4dO6ZNmzZJOhsiOTk5ev7553XTTTeppqZGktS/f3+53e4eehkAACBShX1kZN++fRo3blzb23Lz8vI0btw4Pfroo5Kk6upqBQKBtvXr1q3TmTNnNH/+fCUmJrbdHnzwwR56CQAAIJKFfWTk5ptvVjAY7PTnGzduDLm/Y8eOcJ8CAABcRvhuGgAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmwo6R0tJSTZs2TUlJSXI4HHrzzTcvuM/HH38sn8+n2NhYjRo1SmvXru3OrAAAoA8KO0YaGxs1duxYvfjii11af/jwYd1xxx3KyMhQeXm5HnnkES1YsECvvfZa2MMCAIC+JybcHbKyspSVldXl9WvXrtXw4cNVWFgoSUpJSdG+ffu0fPlyzZgxI9ynBwAAfcwlv2bkk08+UWZmZsi22267Tfv27dPp06c73KepqUkNDQ0hNwAA0Ddd8hipqamRx+MJ2ebxeHTmzBnV1dV1uE9BQYHcbnfbzev1XuoxAQCAkV55N43D4Qi5HwwGO9x+Tn5+vurr69tuVVVVl3xGAABgI+xrRsI1bNgw1dTUhGyrra1VTEyM4uPjO9zH6XTK6XRe6tEAAMD3wCU/MpKWlia/3x+ybfv27ZowYYKuuOKKS/30AADgey7sGDl16pQqKipUUVEh6exbdysqKhQIBCSdPcWSk5PTtj43N1dffPGF8vLyVFlZqaKiIm3YsEGLFy/umVcAAAAiWtinafbt26fJkye33c/Ly5Mk3XPPPdq4caOqq6vbwkSSkpOTVVJSokWLFmnVqlVKSkrSCy+8wNt6AQCAJMkRPHc16fdYQ0OD3G636uvr5XK5rMfpVSOXvGc9AnrRkaenWo8AAD2mq3+/+W4aAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKa6FSOrV69WcnKyYmNj5fP5tHPnzvOu37x5s8aOHasrr7xSiYmJmjNnjk6cONGtgQEAQN8SdowUFxdr4cKFWrp0qcrLy5WRkaGsrCwFAoEO1+/atUs5OTmaO3eu/vnPf2rr1q3au3ev5s2bd9HDAwCAyBd2jKxcuVJz587VvHnzlJKSosLCQnm9Xq1Zs6bD9X/+8581cuRILViwQMnJyZo0aZLuu+8+7du376KHBwAAkS+sGGlublZZWZkyMzNDtmdmZmrPnj0d7pOenq6jR4+qpKREwWBQX375pf70pz9p6tSpnT5PU1OTGhoaQm4AAKBvCitG6urq1NLSIo/HE7Ld4/Gopqamw33S09O1efNmZWdnq1+/fho2bJgGDx6sP/zhD50+T0FBgdxud9vN6/WGMyYAAIgg3bqA1eFwhNwPBoPttp2zf/9+LViwQI8++qjKysq0bds2HT58WLm5uZ0+fn5+vurr69tuVVVV3RkTAABEgJhwFickJCg6OrrdUZDa2tp2R0vOKSgo0MSJE/Xwww9Lkq677joNGDBAGRkZevLJJ5WYmNhuH6fTKafTGc5oAAAgQoV1ZKRfv37y+Xzy+/0h2/1+v9LT0zvc55tvvlFUVOjTREdHSzp7RAUAAFzewj5Nk5eXp/Xr16uoqEiVlZVatGiRAoFA22mX/Px85eTktK2fNm2aXn/9da1Zs0aHDh3S7t27tWDBAv34xz9WUlJSz70SAAAQkcI6TSNJ2dnZOnHihJYtW6bq6mqlpqaqpKREI0aMkCRVV1eHfObI7NmzdfLkSb344ot66KGHNHjwYN1yyy165plneu5VAACAiOUIRsC5koaGBrndbtXX18vlclmP06tGLnnPegT0oiNPd/6WdwCINF39+8130wAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABT3YqR1atXKzk5WbGxsfL5fNq5c+d51zc1NWnp0qUaMWKEnE6nrrnmGhUVFXVrYAAA0LfEhLtDcXGxFi5cqNWrV2vixIlat26dsrKytH//fg0fPrzDfWbOnKkvv/xSGzZs0A9/+EPV1tbqzJkzFz08AACIfI5gMBgMZ4cbb7xR48eP15o1a9q2paSkaPr06SooKGi3ftu2bfrVr36lQ4cOKS4urkvP0dTUpKamprb7DQ0N8nq9qq+vl8vlCmfciDdyyXvWI6AXHXl6qvUIANBjGhoa5Ha7L/j3O6zTNM3NzSorK1NmZmbI9szMTO3Zs6fDfd5++21NmDBBzz77rK666iqNHj1aixcv1rffftvp8xQUFMjtdrfdvF5vOGMCAIAIEtZpmrq6OrW0tMjj8YRs93g8qqmp6XCfQ4cOadeuXYqNjdUbb7yhuro63X///frqq686vW4kPz9feXl5bffPHRkBAAB9T9jXjEiSw+EIuR8MBtttO6e1tVUOh0ObN2+W2+2WJK1cuVJ33XWXVq1apf79+7fbx+l0yul0dmc0AAAQYcI6TZOQkKDo6Oh2R0Fqa2vbHS05JzExUVdddVVbiEhnrzEJBoM6evRoN0YGAAB9SVgx0q9fP/l8Pvn9/pDtfr9f6enpHe4zceJEHT9+XKdOnWrb9tlnnykqKkpXX311N0YGAAB9SdifM5KXl6f169erqKhIlZWVWrRokQKBgHJzcyWdvd4jJyenbf2sWbMUHx+vOXPmaP/+/SotLdXDDz+se++9t8NTNAAA4PIS9jUj2dnZOnHihJYtW6bq6mqlpqaqpKREI0aMkCRVV1crEAi0rR84cKD8fr8eeOABTZgwQfHx8Zo5c6aefPLJnnsVAAAgYoX9OSMWuvo+5b6Izxm5vPA5IwD6kkvyOSMAAAA9jRgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKa6FSOrV69WcnKyYmNj5fP5tHPnzi7tt3v3bsXExOj666/vztMCAIA+KOwYKS4u1sKFC7V06VKVl5crIyNDWVlZCgQC592vvr5eOTk5+ulPf9rtYQEAQN8TdoysXLlSc+fO1bx585SSkqLCwkJ5vV6tWbPmvPvdd999mjVrltLS0i74HE1NTWpoaAi5AQCAvimsGGlublZZWZkyMzNDtmdmZmrPnj2d7vfyyy/r4MGDeuyxx7r0PAUFBXK73W03r9cbzpgAACCChBUjdXV1amlpkcfjCdnu8XhUU1PT4T6ff/65lixZos2bNysmJqZLz5Ofn6/6+vq2W1VVVThjAgCACNK1OvgOh8MRcj8YDLbbJkktLS2aNWuWnnjiCY0ePbrLj+90OuV0OrszGgAAiDBhxUhCQoKio6PbHQWpra1td7REkk6ePKl9+/apvLxcv/vd7yRJra2tCgaDiomJ0fbt23XLLbdcxPgAACDShXWapl+/fvL5fPL7/SHb/X6/0tPT2613uVz6+9//roqKirZbbm6urr32WlVUVOjGG2+8uOkBAEDEC/s0TV5enu6++25NmDBBaWlpeumllxQIBJSbmyvp7PUex44d06ZNmxQVFaXU1NSQ/YcOHarY2Nh22wEAwOUp7BjJzs7WiRMntGzZMlVXVys1NVUlJSUaMWKEJKm6uvqCnzkCAABwjiMYDAath7iQhoYGud1u1dfXy+VyWY/Tq0Yuec96BPSiI09PtR4BAHpMV/9+8900AADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEx1K0ZWr16t5ORkxcbGyufzaefOnZ2uff311zVlyhQNGTJELpdLaWlp+uCDD7o9MAAA6FvCjpHi4mItXLhQS5cuVXl5uTIyMpSVlaVAINDh+tLSUk2ZMkUlJSUqKyvT5MmTNW3aNJWXl1/08AAAIPI5gsFgMJwdbrzxRo0fP15r1qxp25aSkqLp06eroKCgS4/xox/9SNnZ2Xr00Ue7tL6hoUFut1v19fVyuVzhjBvxRi55z3oE9KIjT0+1HgEAekxX/36HdWSkublZZWVlyszMDNmemZmpPXv2dOkxWltbdfLkScXFxXW6pqmpSQ0NDSE3AADQN4UVI3V1dWppaZHH4wnZ7vF4VFNT06XHWLFihRobGzVz5sxO1xQUFMjtdrfdvF5vOGMCAIAI0q0LWB0OR8j9YDDYbltHtmzZoscff1zFxcUaOnRop+vy8/NVX1/fdquqqurOmAAAIALEhLM4ISFB0dHR7Y6C1NbWtjta8l3FxcWaO3eutm7dqltvvfW8a51Op5xOZzijAQCACBXWkZF+/frJ5/PJ7/eHbPf7/UpPT+90vy1btmj27Nl65ZVXNHUqF+gBAID/CevIiCTl5eXp7rvv1oQJE5SWlqaXXnpJgUBAubm5ks6eYjl27Jg2bdok6WyI5OTk6Pnnn9dNN93UdlSlf//+crvdPfhSAABAJAo7RrKzs3XixAktW7ZM1dXVSk1NVUlJiUaMGCFJqq6uDvnMkXXr1unMmTOaP3++5s+f37b9nnvu0caNGy/+FQAAgIgW9ueMWOBzRnC54HNGAPQll+RzRgAAAHoaMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEzFWA8AAJerkUvesx4BvejI01OtR/je4sgIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAw1a0YWb16tZKTkxUbGyufz6edO3eed/3HH38sn8+n2NhYjRo1SmvXru3WsAAAoO8JO0aKi4u1cOFCLV26VOXl5crIyFBWVpYCgUCH6w8fPqw77rhDGRkZKi8v1yOPPKIFCxbotddeu+jhAQBA5As7RlauXKm5c+dq3rx5SklJUWFhobxer9asWdPh+rVr12r48OEqLCxUSkqK5s2bp3vvvVfLly+/6OEBAEDkC+vj4Jubm1VWVqYlS5aEbM/MzNSePXs63OeTTz5RZmZmyLbbbrtNGzZs0OnTp3XFFVe026epqUlNTU1t9+vr6yVJDQ0N4YzbJ7Q2fWM9AnrR5fjf+OWM3+/Ly+X4+33uNQeDwfOuCytG6urq1NLSIo/HE7Ld4/Gopqamw31qamo6XH/mzBnV1dUpMTGx3T4FBQV64okn2m33er3hjAtEHHeh9QQALpXL+ff75MmTcrvdnf68W1+U53A4Qu4Hg8F22y60vqPt5+Tn5ysvL6/tfmtrq7766ivFx8ef93nQNzQ0NMjr9aqqqkoul8t6HAA9iN/vy0swGNTJkyeVlJR03nVhxUhCQoKio6PbHQWpra1td/TjnGHDhnW4PiYmRvHx8R3u43Q65XQ6Q7YNHjw4nFHRB7hcLv5nBfRR/H5fPs53ROScsC5g7devn3w+n/x+f8h2v9+v9PT0DvdJS0trt3779u2aMGFCh9eLAACAy0vY76bJy8vT+vXrVVRUpMrKSi1atEiBQEC5ubmSzp5iycnJaVufm5urL774Qnl5eaqsrFRRUZE2bNigxYsX99yrAAAAESvsa0ays7N14sQJLVu2TNXV1UpNTVVJSYlGjBghSaqurg75zJHk5GSVlJRo0aJFWrVqlZKSkvTCCy9oxowZPfcq0Kc4nU499thj7U7VAYh8/H6jI47ghd5vAwAAcAnx3TQAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAVLc+Dh7oSUePHtWaNWu0Z88e1dTUyOFwyOPxKD09Xbm5uXwnEQD0cby1F6Z27dqlrKwseb1eZWZmyuPxKBgMqra2Vn6/X1VVVXr//fc1ceJE61EB9LCqqio99thjKioqsh4FxogRmLrhhhs0adIkPffccx3+fNGiRdq1a5f27t3by5MBuNT++te/avz48WppabEeBcaIEZjq37+/KioqdO2113b483/9618aN26cvv32216eDMDFevvtt8/780OHDumhhx4iRsA1I7CVmJioPXv2dBojn3zyiRITE3t5KgA9Yfr06XI4HDrfv3kdDkcvToTvK2IEphYvXqzc3FyVlZVpypQp8ng8cjgcqqmpkd/v1/r161VYWGg9JoBuSExM1KpVqzR9+vQOf15RUSGfz9e7Q+F7iRiBqfvvv1/x8fF67rnntG7durbDtdHR0fL5fNq0aZNmzpxpPCWA7vD5fPr00087jZELHTXB5YNrRvC9cfr0adXV1UmSEhISdMUVVxhPBOBi7Ny5U42Njbr99ts7/HljY6P27dunn/zkJ708Gb5viBEAAGCKT2AFAACmiBEAAGCKGAEAAKaIEQAAYIoYARCWYDCo3/72t4qLi5PD4VBFRcV51x85ciRk3Y4dO+RwOPT1119f8lkBRAY+ZwRAWLZt26aNGzdqx44dGjVqlBISEs673uv1qrq6+oLrAFy+iBEAYTl48KASExOVnp7epfXR0dEaNmxYj87Q3Nysfv369ehjArDDaRoAXTZ79mw98MADCgQCcjgcGjlypLZt26ZJkyZp8ODBio+P189+9jMdPHiwbZ/vnqb5rscff1zXX399yLbCwkKNHDky5HmnT5+ugoICJSUlafTo0ZKkY8eOKTs7Wz/4wQ8UHx+vO++8U0eOHOnhVw3gUiNGAHTZ888/r2XLlunqq69WdXW19u7dq8bGRuXl5Wnv3r368MMPFRUVpZ///OdqbW3t0ef+8MMPVVlZKb/fr3fffVfffPONJk+erIEDB6q0tFS7du3SwIEDdfvtt6u5ublHnxvApcVpGgBd5na7NWjQoJBTLzNmzAhZs2HDBg0dOlT79+9Xampqjz33gAEDtH79+rbTM0VFRYqKitL69evbvvn15Zdf1uDBg7Vjxw5lZmb22HMDuLQ4MgLgohw8eFCzZs3SqFGj5HK5lJycLEkKBAI9+jxjxowJuU6krKxMBw4c0KBBgzRw4EANHDhQcXFx+u9//xtymgjA9x9HRgBclGnTpsnr9eqPf/yjkpKS1NraqtTU1C6fKomKimr3za2nT59ut27AgAEh91tbW+Xz+bR58+Z2a4cMGRLGKwBgjRgB0G0nTpxQZWWl1q1bp4yMDEnSrl27wnqMIUOGqKamRsFgsO10y4U+u0SSxo8fr+LiYg0dOlQulyvs2QF8f3CaBkC3nXsXy0svvaQDBw7oo48+Ul5eXliPcfPNN+s///mPnn32WR08eFCrVq3S+++/f8H9fv3rXyshIUF33nmndu7cqcOHD+vjjz/Wgw8+qKNHj3b3JQEwQIwA6LaoqCi9+uqrKisrU2pqqhYtWqT/+7//C+sxUlJStHr1aq1atUpjx47VX/7yFy1evPiC+1155ZUqLS3V8OHD9Ytf/EIpKSm699579e2333KkBIgwjuB3T9YCAAD0Io6MAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFP/D6NORD4My+OQAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "caxton['failure'].value_counts().plot(kind='bar', y='failure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, data, transform=None, max_frames=100, frame_interval=3):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.max_frames = max_frames\n",
    "        self.frame_interval = frame_interval\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        print_id = self.data.iloc[idx]['print_id']\n",
    "        img_path = self.data.iloc[idx]['img_path']\n",
    "        label = 1 if print_id >= 183 else 0\n",
    "        \n",
    "        print_dir = os.path.dirname(os.path.join('data', img_path))\n",
    "        frame_files = sorted([f for f in os.listdir(print_dir) if f.endswith('.jpg')])\n",
    "        \n",
    "        # Sampling strategy\n",
    "        total_frames = len(frame_files)\n",
    "        if total_frames <= self.max_frames:\n",
    "            sampled_files = frame_files[::self.frame_interval][:self.max_frames]\n",
    "        else:\n",
    "            interval = max(self.frame_interval, total_frames // self.max_frames)\n",
    "            sampled_files = frame_files[::interval][:self.max_frames]\n",
    "        \n",
    "        frames = []\n",
    "        for frame_file in sampled_files:\n",
    "            full_img_path = os.path.join(print_dir, frame_file)\n",
    "            img = Image.open(full_img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            frames.append(img)\n",
    "\n",
    "        frames_tensor = torch.stack(frames)\n",
    "        return frames_tensor, label, len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 2\n",
    "accumulation_steps = 4  # Number of steps to accumulate gradients\n",
    "\n",
    "# CNN hyperparameters\n",
    "input_channels = 3  # RGB images\n",
    "num_classes = 1 # Binary classification (normal vs failure)\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 3\n",
    "dropout_rate = 0.5\n",
    "\n",
    "# Define image dimensions\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((img_height, img_width)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# LSTM hyperparameters\n",
    "lstm_hidden_size = 128\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(caxton, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.25, random_state=42) \n",
    "\n",
    "# Create datasets\n",
    "max_frames = 300  # Adjust this based on your GPU memory and desired maximum sequence length\n",
    "train_dataset = VideoDataset(train_df, transform=transform, max_frames=max_frames)\n",
    "val_dataset = VideoDataset(val_df, transform=transform, max_frames=max_frames)\n",
    "test_dataset = VideoDataset(test_df, transform=transform, max_frames=max_frames)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=3, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        \n",
    "        # Use a smaller pre-trained model\n",
    "        self.efficientnet = models.efficientnet_b0(weights='DEFAULT')\n",
    "        \n",
    "        # Remove the last fully connected layer\n",
    "        self.features = nn.Sequential(*list(self.efficientnet.children())[:-1])\n",
    "        \n",
    "        # Freeze the CNN parameters\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "    \n",
    "class SequenceProcessor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, dropout_rate):\n",
    "        super(SequenceProcessor, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=1, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (h_n, c_n) = self.lstm(packed_input)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # Use the last output for each sequence\n",
    "        last_output = output[torch.arange(output.size(0)), lengths - 1]\n",
    "        out = self.fc(last_output)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class VideoCNNLSTM(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes, lstm_hidden_size, dropout_rate):\n",
    "        super(VideoCNNLSTM, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = FeatureExtractor()\n",
    "        \n",
    "        # Get the number of features from the CNN\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, input_channels, 224, 224)\n",
    "            cnn_output = self.feature_extractor(dummy_input)\n",
    "            cnn_output_size = cnn_output.view(-1).shape[0]\n",
    "        \n",
    "        self.sequence_processor = SequenceProcessor(\n",
    "            input_size=cnn_output_size,\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            num_classes=num_classes,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        batch_size, max_len, C, H, W = x.size()\n",
    "        c_in = x.view(batch_size * max_len, C, H, W)\n",
    "        \n",
    "        # CNN forward pass\n",
    "        c_out = self.feature_extractor(c_in)\n",
    "        \n",
    "        # Reshape for LSTM input\n",
    "        r_in = c_out.view(batch_size, max_len, -1)\n",
    "        \n",
    "        # Sequence processing\n",
    "        out = self.sequence_processor(r_in, lengths)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /home/jadhaiza/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 20.5M/20.5M [00:01<00:00, 11.5MB/s]\n",
      "/home/jadhaiza/miniconda3/envs/ka/lib/python3.12/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "Epoch 1/3:   0%|                                                                                         | 41/381682 [04:36<714:58:38,  6.74s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/home/jadhaiza/miniconda3/envs/ka/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jadhaiza/miniconda3/envs/ka/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jadhaiza/miniconda3/envs/ka/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 316, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jadhaiza/miniconda3/envs/ka/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 173, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jadhaiza/miniconda3/envs/ka/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 141, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jadhaiza/miniconda3/envs/ka/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 213, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [220, 3, 224, 224] at entry 0 and [300, 3, 224, 224] at entry 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m train_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (inputs, labels, lengths) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_loader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     21\u001b[0m     inputs, labels, lengths \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device), lengths\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     23\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs, lengths)\n",
      "File \u001b[0;32m~/miniconda3/envs/ka/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ka/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/ka/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/ka/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     data\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/ka/lib/python3.12/site-packages/torch/_utils.py:705\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/home/jadhaiza/miniconda3/envs/ka/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jadhaiza/miniconda3/envs/ka/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jadhaiza/miniconda3/envs/ka/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 316, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jadhaiza/miniconda3/envs/ka/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 173, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jadhaiza/miniconda3/envs/ka/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 141, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jadhaiza/miniconda3/envs/ka/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 213, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [220, 3, 224, 224] at entry 0 and [300, 3, 224, 224] at entry 1\n"
     ]
    }
   ],
   "source": [
    "model = VideoCNNLSTM(\n",
    "    input_channels=input_channels,\n",
    "    num_classes=num_classes,\n",
    "    lstm_hidden_size=lstm_hidden_size,\n",
    "    dropout_rate=dropout_rate\n",
    ").to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    for batch, (inputs, labels, lengths) in tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        inputs, labels, lengths = inputs.to(device), labels.to(device), lengths.to(device)\n",
    "        \n",
    "        outputs = model(inputs, lengths)\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        if (batch + 1) % accumulation_steps == 0 or (batch + 1) == len(train_loader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        train_loss += loss.item() * accumulation_steps\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).squeeze()\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, lengths in tqdm(val_loader, desc=\"Validation\"):\n",
    "            inputs, labels, lengths = inputs.to(device), labels.to(device), lengths.to(device)\n",
    "            outputs = model(inputs, lengths)\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).squeeze()\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:48<00:00,  2.42s/it]\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels, lengths in tqdm(test_loader, desc=\"Testing\"):\n",
    "        inputs, labels, lengths = inputs.to(device), labels.to(device), lengths.to(device)\n",
    "        outputs = model(inputs, lengths)\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).squeeze()\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
