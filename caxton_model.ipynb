{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "%pip install pandas numpy torch opencv-python torchvision tqdm scikit_learn\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "import cv2\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>flow_rate</th>\n",
       "      <th>feed_rate</th>\n",
       "      <th>z_offset</th>\n",
       "      <th>target_hotend</th>\n",
       "      <th>hotend</th>\n",
       "      <th>bed</th>\n",
       "      <th>nozzle_tip_x</th>\n",
       "      <th>nozzle_tip_y</th>\n",
       "      <th>print_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caxton_dataset/print0/image-1.jpg</td>\n",
       "      <td>2020-10-08T13:12:48-02</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>204.34</td>\n",
       "      <td>65.66</td>\n",
       "      <td>531</td>\n",
       "      <td>554</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>caxton_dataset/print0/image-2.jpg</td>\n",
       "      <td>2020-10-08T13:12:48-48</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>204.34</td>\n",
       "      <td>65.66</td>\n",
       "      <td>531</td>\n",
       "      <td>554</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>caxton_dataset/print0/image-3.jpg</td>\n",
       "      <td>2020-10-08T13:12:48-94</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>204.13</td>\n",
       "      <td>65.74</td>\n",
       "      <td>531</td>\n",
       "      <td>554</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            img_path               timestamp  flow_rate  \\\n",
       "0  caxton_dataset/print0/image-1.jpg  2020-10-08T13:12:48-02        100   \n",
       "1  caxton_dataset/print0/image-2.jpg  2020-10-08T13:12:48-48        100   \n",
       "2  caxton_dataset/print0/image-3.jpg  2020-10-08T13:12:48-94        100   \n",
       "\n",
       "   feed_rate  z_offset  target_hotend  hotend    bed  nozzle_tip_x  \\\n",
       "0        100       0.0          205.0  204.34  65.66           531   \n",
       "1        100       0.0          205.0  204.34  65.66           531   \n",
       "2        100       0.0          205.0  204.13  65.74           531   \n",
       "\n",
       "   nozzle_tip_y  print_id  \n",
       "0           554         0  \n",
       "1           554         0  \n",
       "2           554         0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = 'caxton_dataset'\n",
    "\n",
    "caxton = pd.read_csv(os.path.join(dataset_path,'caxton_dataset_full.csv'))\n",
    "prints = os.listdir(dataset_path)\n",
    "prints = [int(pr.split('t')[1]) if pr.startswith('print') and pr != 'print107' else pr for pr in prints]\n",
    "caxton = caxton[caxton['print_id'].isin(prints)]\n",
    "caxton.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 28 18:34:14 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2080 Ti     On  |   00000000:1A:00.0 Off |                  N/A |\n",
      "| 30%   30C    P8             21W /  250W |       0MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 0.00 MB\n",
      "GPU memory cached: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "clear_gpu_memory()\n",
    "\n",
    "print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1e6:.2f} MB\")\n",
    "print(f\"GPU memory cached: {torch.cuda.memory_reserved()/1e6:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_label(row):\n",
    "    print_id = row['print_id']\n",
    "    return 1 if print_id >= 183 else 0  # 1 for failure, 0 for non-failure\n",
    "\n",
    "caxton['failure'] = caxton.apply(create_binary_label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='failure'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "caxton['failure'].value_counts().plot(kind='bar', y='failure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.data.iloc[index]['img_path']\n",
    "        failure = self.data.iloc[index]['failure']\n",
    "\n",
    "        try:\n",
    "            # Check if file exists\n",
    "            if not os.path.exists(img_path):\n",
    "                # raise FileNotFoundError(f\"Image file not found: {img_path}\")\n",
    "                img = torch.zeros((3, 224, 224), dtype=torch.uint8)  \n",
    "                failure = 0\n",
    "                return img, failure\n",
    "\n",
    "            # Read image\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            # Check if image is None (indicating a read error)\n",
    "            if img is None:\n",
    "                raise IOError(f\"Error reading image file: {img_path}\")\n",
    "\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {str(e)}\")\n",
    "            # Return a placeholder image and the failure label\n",
    "            img = np.zeros((224, 224, 3), dtype=np.uint8)  \n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "\n",
    "        return img, failure\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# CNN hyperparameters\n",
    "input_channels = 3  # RGB images\n",
    "num_classes = 1 # Binary classification (normal vs failure)\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 50\n",
    "dropout_rate = 0.5\n",
    "\n",
    "# Define image dimensions\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((img_height, img_width)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(caxton, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.25, random_state=42) \n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ImageDataset(train_df, transform)\n",
    "val_dataset = ImageDataset(val_df, transform)\n",
    "test_dataset = ImageDataset(test_df, transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return torch.flatten(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pic should be Tensor or ndarray. Got <class 'PIL.Image.Image'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m train_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      9\u001b[0m train_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_loader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     12\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Forward Pass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ka/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ka/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/ka/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/ka/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/ka/lib/python3.12/site-packages/torch/utils/data/dataset.py:419\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[49], line 28\u001b[0m, in \u001b[0;36mCaxtonDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     25\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 28\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "File \u001b[0;32m~/miniconda3/envs/ka/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/miniconda3/envs/ka/lib/python3.12/site-packages/torchvision/transforms/transforms.py:234\u001b[0m, in \u001b[0;36mToPILImage.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    226\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mto_pil_image(pic, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode)\n",
      "File \u001b[0;32m~/miniconda3/envs/ka/lib/python3.12/site-packages/torchvision/transforms/functional.py:268\u001b[0m, in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    266\u001b[0m     pic \u001b[38;5;241m=\u001b[39m pic\u001b[38;5;241m.\u001b[39mnumpy(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pic, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should be Tensor or ndarray. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pic)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;66;03m# if 2D image, add channel dimension (HWC)\u001b[39;00m\n\u001b[1;32m    272\u001b[0m     pic \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(pic, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: pic should be Tensor or ndarray. Got <class 'PIL.Image.Image'>."
     ]
    }
   ],
   "source": [
    "model = model(input_channels, num_classes).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch, (inputs, labels) in tqdm(enumerate(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward Pass\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels.float())\n",
    "        \n",
    "        # Backward Pass and Optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        predicted = (outputs > 0).float()\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels.float())\n",
    "            val_loss += loss.item()\n",
    "            predicted = (outputs > 0).float()\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Test\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = loss_fn(outputs, labels.float())\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        predicted = (outputs > 0).float()\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = 100 * test_correct / test_total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
